{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6cebc3-cde6-41ba-bb41-1832faff98cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://source@storageecobici.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/source\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a29fd23-fa22-4a3c-98c3-150494ed574b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/source/2021-01.csv', name='2021-01.csv', size=14990775, modificationTime=1704820949000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-02.csv', name='2021-02.csv', size=16881861, modificationTime=1704820942000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-03.csv', name='2021-03.csv', size=20658110, modificationTime=1704820969000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-04.csv', name='2021-04.csv', size=20523655, modificationTime=1704820976000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-05.csv', name='2021-05.csv', size=22077522, modificationTime=1704820973000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-06.csv', name='2021-06.csv', size=21463427, modificationTime=1704821076000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-07.csv', name='2021-07.csv', size=20587278, modificationTime=1704821078000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-08.csv', name='2021-08.csv', size=20248095, modificationTime=1704821097000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-09.csv', name='2021-09.csv', size=20259543, modificationTime=1704821086000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-10.csv', name='2021-10.csv', size=24337008, modificationTime=1704821111000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-11.csv', name='2021-11.csv', size=25470047, modificationTime=1704821155000),\n",
       " FileInfo(path='dbfs:/mnt/source/2021-12.csv', name='2021-12.csv', size=24404114, modificationTime=1704821155000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-01.csv', name='2022-01.csv', size=22265457, modificationTime=1704820511000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-02.csv', name='2022-02.csv', size=24599264, modificationTime=1704820528000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-03.csv', name='2022-03.csv', size=31617831, modificationTime=1704820567000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-04.csv', name='2022-04.csv', size=29687706, modificationTime=1704820571000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-05.csv', name='2022-05.csv', size=33676902, modificationTime=1704820721000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-06.csv', name='2022-06.csv', size=29852283, modificationTime=1704820715000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-07.csv', name='2022-07.csv', size=22846559, modificationTime=1704820668000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-08.csv', name='2022-08.csv', size=21174202, modificationTime=1704820692000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-09.csv', name='2022-09.csv', size=18850059, modificationTime=1704820682000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-10.csv', name='2022-10.csv', size=14244697, modificationTime=1704820738000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-11.csv', name='2022-11.csv', size=25976478, modificationTime=1704820768000),\n",
       " FileInfo(path='dbfs:/mnt/source/2022-12.csv', name='2022-12.csv', size=26529135, modificationTime=1704820777000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-01.csv', name='2023-01.csv', size=36066543, modificationTime=1703794935000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-02.csv', name='2023-02.csv', size=40887810, modificationTime=1703794956000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-03.csv', name='2023-03.csv', size=50275032, modificationTime=1703795036000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-04.csv', name='2023-04.csv', size=49988957, modificationTime=1703795236000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-05.csv', name='2023-05.csv', size=58080469, modificationTime=1703795089000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-06.csv', name='2023-06.csv', size=62051320, modificationTime=1703795355000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-07.csv', name='2023-07.csv', size=61419211, modificationTime=1703795382000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-08.csv', name='2023-08.csv', size=68143405, modificationTime=1703795386000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-09.csv', name='2023-09.csv', size=73083994, modificationTime=1703795489000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-10.csv', name='2023-10.csv', size=86097704, modificationTime=1703795547000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-11.csv', name='2023-11.csv', size=86631366, modificationTime=1703795568000),\n",
       " FileInfo(path='dbfs:/mnt/source/2023-12.csv', name='2023-12.csv', size=75392721, modificationTime=1704820531000)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53325ee5-1ddd-4200-8828-ea35d26b96c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://bronze@storageecobici.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/bronze\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76edb5a3-84af-4b5f-b5ed-86d7f0d50ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://silver@storageecobici.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/silver\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6a5614-f8f3-4c83-beca-5511673971b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://gold@storageecobici.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/gold\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5b8c0c-d8fa-485e-ab8e-15fdba33120d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://cicloestaciones@storageecobici.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/cicloestaciones\",\n",
    "  extra_configs = configs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mount_Storage",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
