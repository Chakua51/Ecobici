{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8af373-bd75-4b84-9d2e-566e9b79bbdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435158c9-054c-4218-86f4-bea3d9ce0ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"merge_csv_files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e41229-9ed7-40b8-b887-c0518ab9b562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ruta del contenedor en Azure Databricks\n",
    "container_path = \"/mnt/source/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa533cc-34d0-4cab-8aa7-d7b822333792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos en el contenedor\n",
    "files = [file.path for file in dbutils.fs.ls(container_path) if file.path.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b9abda-a895-4c73-a60e-dba7031958a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos combinados exitosamente. Resultado guardado en: /mnt/bronze/Eco_Bici_2021_2023.csv\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay archivos para procesar\n",
    "if not files:\n",
    "    print(\"No hay archivos CSV para procesar en el contenedor.\")\n",
    "else:\n",
    "    # Leer y combinar archivos CSV\n",
    "    combined_df = None\n",
    "\n",
    "    for file in files:\n",
    "        df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.union(df)\n",
    "\n",
    "    # Reducir el número de particiones a 1\n",
    "    combined_df = combined_df.coalesce(1)\n",
    "\n",
    "    # Guardar el DataFrame combinado en un solo archivo CSV\n",
    "    combined_output_path = \"/mnt/bronze/Eco_Bici_2021_2023.csv\"\n",
    "    df_sin_nulos = combined_df.dropna(how=\"any\")\n",
    "    df_sin_nulos.write.parquet(combined_output_path, mode=\"overwrite\")\n",
    "\n",
    "    print(f\"Archivos combinados exitosamente. Resultado guardado en: {combined_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Union_CSV",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
